---
body: ''
content_type: resource
draft: false
file: /courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/ocw_18s096_lecture05-part3-new_2023jan27_360p_16_9.mp4
file_size: 67929351
file_type: video/mp4
image_metadata:
  caption: ''
  credit: ''
  image-alt: ''
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
resourcetype: Video
title: 'Lecture 5 Part 3: Differentiation on Computational Graphs'
uid: 6c74b829-6a1a-4c23-bdf3-65eb105ae7b9
video_files:
  archive_url: ''
  video_captions_file: /courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/ocw_18s096_lecture05-part3-new_2023jan27_captions.vtt
  video_thumbnail_file: https://img.youtube.com/vi/r9_5dxtDTOk/default.jpg
  video_transcript_file: /courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/ocw_18s096_lecture05-part3-new_2023jan27_transcript.pdf
video_metadata:
  video_speakers: ''
  video_tags: chain rule, propagate derivatives, forward-mode automatic differentiation,
    reverse mode automatic differentiation
  youtube_description: "MIT 18.S096 Matrix Calculus For Machine Learning And Beyond,\
    \ IAP 2023\nInstructors: Alan Edelman, Steven G. Johnson\n\nView the complete\
    \ course: https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/\n\
    YouTube Playlist: https://www.youtube.com/playlist?list=PLUl4u3cNGP62EaLLH92E_VCN4izBKK6OE\n\
    \nDescription: A very general way to think about the chain rule is to view computations\
    \ as flowing through \"graphs\" consisting of nodes (intermediate values) connected\
    \ by edges (functions acting on those values). When we propagate derivatives through\
    \ the graph from inputs to outputs, we get the structure of forward-mode automatic\
    \ differentiation; going from outputs to inputs yields reverse mode, which we\
    \ will return to in lecture 8.\n\nLicense: Creative Commons BY-NC-SA\nMore information\
    \ at https://ocw.mit.edu/terms\nMore courses at https://ocw.mit.edu\nSupport OCW\
    \ at http://ow.ly/a1If50zVRlQ\n\nWe encourage constructive comments and discussion\
    \ on OCW\u2019s YouTube and other social media channels. Personal attacks, hate\
    \ speech, trolling, and inappropriate comments are not allowed and may be removed.\
    \ More details at https://ocw.mit.edu/comments."
  youtube_id: r9_5dxtDTOk
---
**Description:** A very general way to think about the chain rule is to view computations as flowing through "graphs" consisting of nodes (intermediate values) connected by edges (functions acting on those values). When we propagate derivatives through the graph from inputs to outputs, we get the structure of forward-mode automatic differentiation; going from outputs to inputs yields reverse mode, which we will return to in lecture 8.

**Instructors:** Alan Edelman, Steven G. Johnson